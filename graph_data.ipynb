{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from utils import load_all_data, preprocess\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "from dgl.nn import GraphConv, SAGEConv\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, features, relations, labels = load_all_data()\n",
    "X_train, X_test, y_train, y_test = preprocess(features, labels, include_unlabeled=True, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>languages_JavaScript</th>\n",
       "      <th>languages_Python</th>\n",
       "      <th>languages_TypeScript</th>\n",
       "      <th>languages_HTML</th>\n",
       "      <th>languages_Go</th>\n",
       "      <th>languages_Java</th>\n",
       "      <th>languages_PHP</th>\n",
       "      <th>languages_Jupyter Notebook</th>\n",
       "      <th>languages_CSS</th>\n",
       "      <th>languages_Shell</th>\n",
       "      <th>...</th>\n",
       "      <th>topics_algorithm</th>\n",
       "      <th>topics_plugin</th>\n",
       "      <th>topics_vercel</th>\n",
       "      <th>topics_music</th>\n",
       "      <th>topics_vue3</th>\n",
       "      <th>topics_security</th>\n",
       "      <th>topics_cryptocurrency</th>\n",
       "      <th>topics_data</th>\n",
       "      <th>topics_rails</th>\n",
       "      <th>topics_twitter</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>username</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>007jedgar</th>\n",
       "      <td>0.701</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00Kai0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00imvj00</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.248</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0101011</th>\n",
       "      <td>0.014</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.002</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0326</th>\n",
       "      <td>0.643</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yasik</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yysu</th>\n",
       "      <td>0.366</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.173</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zhouzi</th>\n",
       "      <td>0.347</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zhukovgreen</th>\n",
       "      <td>0.009</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zomervinicius</th>\n",
       "      <td>0.631</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28727 rows Ã— 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               languages_JavaScript  languages_Python  languages_TypeScript  \\\n",
       "username                                                                      \n",
       "007jedgar                     0.701             0.000                 0.165   \n",
       "00Kai0                        0.000             0.875                 0.000   \n",
       "00imvj00                      0.005             0.014                 0.008   \n",
       "0101011                       0.014             0.588                 0.000   \n",
       "0326                          0.643             0.000                 0.137   \n",
       "...                             ...               ...                   ...   \n",
       "yasik                         0.000             0.000                 0.000   \n",
       "yysu                          0.366             0.184                 0.000   \n",
       "zhouzi                        0.347             0.000                 0.637   \n",
       "zhukovgreen                   0.009             0.750                 0.218   \n",
       "zomervinicius                 0.631             0.000                 0.087   \n",
       "\n",
       "               languages_HTML  languages_Go  languages_Java  languages_PHP  \\\n",
       "username                                                                     \n",
       "007jedgar               0.009         0.000           0.003          0.000   \n",
       "00Kai0                  0.006         0.000           0.000          0.000   \n",
       "00imvj00                0.001         0.010           0.254          0.003   \n",
       "0101011                 0.080         0.000           0.000          0.000   \n",
       "0326                    0.086         0.000           0.000          0.000   \n",
       "...                       ...           ...             ...            ...   \n",
       "yasik                   0.000         0.981           0.000          0.000   \n",
       "yysu                    0.000         0.200           0.000          0.000   \n",
       "zhouzi                  0.002         0.000           0.000          0.000   \n",
       "zhukovgreen             0.000         0.000           0.000          0.000   \n",
       "zomervinicius           0.000         0.000           0.000          0.000   \n",
       "\n",
       "               languages_Jupyter Notebook  languages_CSS  languages_Shell  \\\n",
       "username                                                                    \n",
       "007jedgar                             0.0          0.112            0.000   \n",
       "00Kai0                                0.0          0.015            0.000   \n",
       "00imvj00                              0.0          0.003            0.248   \n",
       "0101011                               0.0          0.038            0.002   \n",
       "0326                                  0.0          0.024            0.000   \n",
       "...                                   ...            ...              ...   \n",
       "yasik                                 0.0          0.000            0.007   \n",
       "yysu                                  0.0          0.000            0.173   \n",
       "zhouzi                                0.0          0.001            0.000   \n",
       "zhukovgreen                           0.0          0.008            0.003   \n",
       "zomervinicius                         0.2          0.082            0.000   \n",
       "\n",
       "               ...  topics_algorithm  topics_plugin  topics_vercel  \\\n",
       "username       ...                                                   \n",
       "007jedgar      ...                 0              0              0   \n",
       "00Kai0         ...                 0              0              0   \n",
       "00imvj00       ...                 0              0              0   \n",
       "0101011        ...                 0              0              0   \n",
       "0326           ...                 0              0              0   \n",
       "...            ...               ...            ...            ...   \n",
       "yasik          ...                 0              0              0   \n",
       "yysu           ...                 0              0              0   \n",
       "zhouzi         ...                 0              0              0   \n",
       "zhukovgreen    ...                 0              0              0   \n",
       "zomervinicius  ...                 0              0              0   \n",
       "\n",
       "               topics_music  topics_vue3  topics_security  \\\n",
       "username                                                    \n",
       "007jedgar                 0            0                0   \n",
       "00Kai0                    0            0                0   \n",
       "00imvj00                  0            0                0   \n",
       "0101011                   0            0                0   \n",
       "0326                      0            0                0   \n",
       "...                     ...          ...              ...   \n",
       "yasik                     0            0                0   \n",
       "yysu                      0            0                0   \n",
       "zhouzi                    0            0                0   \n",
       "zhukovgreen               0            0                0   \n",
       "zomervinicius             0            0                0   \n",
       "\n",
       "               topics_cryptocurrency  topics_data  topics_rails  \\\n",
       "username                                                          \n",
       "007jedgar                          0            0             0   \n",
       "00Kai0                             0            0             0   \n",
       "00imvj00                           0            0             0   \n",
       "0101011                            0            0             0   \n",
       "0326                               0            0             0   \n",
       "...                              ...          ...           ...   \n",
       "yasik                              0            0             0   \n",
       "yysu                               0            0             0   \n",
       "zhouzi                             0            0             0   \n",
       "zhukovgreen                        0            0             0   \n",
       "zomervinicius                      0            0             0   \n",
       "\n",
       "               topics_twitter  \n",
       "username                       \n",
       "007jedgar                   0  \n",
       "00Kai0                      0  \n",
       "00imvj00                    0  \n",
       "0101011                     0  \n",
       "0326                        0  \n",
       "...                       ...  \n",
       "yasik                       0  \n",
       "yysu                        0  \n",
       "zhouzi                      0  \n",
       "zhukovgreen                 0  \n",
       "zomervinicius               0  \n",
       "\n",
       "[28727 rows x 200 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GithubDataset(DGLDataset):\n",
    "    def __init__(self, undirected=False):\n",
    "        self.undirected = undirected\n",
    "        super().__init__(name='github_dataset')\n",
    "\n",
    "    def process(self):\n",
    "        _, features, relations, labels = load_all_data()\n",
    "        X_train, X_test, y_train, y_test = preprocess(features, labels, include_unlabeled=True, test_size=0.2)\n",
    "        features = pd.concat([X_train, X_test], axis=0)\n",
    "        features = features.sort_index()\n",
    "        labels = pd.concat([y_train, y_test], axis=0)\n",
    "        labels = labels.sort_index()\n",
    "\n",
    "        all_users = list(set(X_train.index).union(X_test.index))\n",
    "        l_user = LabelEncoder()\n",
    "        l_user.fit(all_users)\n",
    "\n",
    "        l_label = LabelEncoder()\n",
    "        labels = l_label.fit_transform(labels)\n",
    "\n",
    "\n",
    "        src = l_user.transform(relations['following'])\n",
    "        dest = l_user.transform(relations['follow'])\n",
    "\n",
    "        train_index = l_user.transform(X_train.index)\n",
    "        test_index = l_user.transform(X_test.index)\n",
    "\n",
    "\n",
    "        edges_src = torch.from_numpy(src)\n",
    "        edges_dst = torch.from_numpy(dest)\n",
    "        \n",
    "        self.l_user = l_user\n",
    "        self.l_label = l_label\n",
    "        self.num_classes = len(l_label.classes_) - 1\n",
    "\n",
    "        self.graph = dgl.graph((edges_src, edges_dst), num_nodes=features.shape[0])\n",
    "        if self.undirected:\n",
    "            self.graph.add_edges(edges_dst, edges_src)\n",
    "        \n",
    "        node_features = torch.from_numpy(features.to_numpy())\n",
    "        node_labels = torch.from_numpy(labels)\n",
    "        self.graph.ndata['feat'] = node_features\n",
    "        self.graph.ndata['label'] = node_labels\n",
    "\n",
    "        n_nodes = features.shape[0]\n",
    "        n_train = int(n_nodes * 0.8)\n",
    "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        train_mask[train_index] = True\n",
    "        test_mask[test_index] = True\n",
    "\n",
    "        labeled_mask = (node_labels < self.num_classes)\n",
    "        train_labeled_mask = (labeled_mask) & (train_mask)\n",
    "        test_labeled_mask = (labeled_mask) & (test_mask)\n",
    "        print(train_labeled_mask.sum())\n",
    "        print(test_labeled_mask.sum())\n",
    "\n",
    "\n",
    "        self.graph.ndata['train_mask'] = train_mask\n",
    "        self.graph.ndata['test_mask'] = test_mask\n",
    "        self.graph.ndata['label_mask'] = labeled_mask\n",
    "        self.graph.ndata['train_label_mask'] = train_labeled_mask\n",
    "        self.graph.ndata['test_label_mask'] = test_labeled_mask\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.graph\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7046)\n",
      "tensor(1818)\n",
      "Graph(num_nodes=35909, num_edges=306717,\n",
      "      ndata_schemes={'feat': Scheme(shape=(200,), dtype=torch.float64), 'label': Scheme(shape=(), dtype=torch.int64), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool), 'label_mask': Scheme(shape=(), dtype=torch.bool), 'train_label_mask': Scheme(shape=(), dtype=torch.bool), 'test_label_mask': Scheme(shape=(), dtype=torch.bool)}\n",
      "      edata_schemes={})\n"
     ]
    }
   ],
   "source": [
    "dataset = GithubDataset(undirected=True)\n",
    "graph = dataset[0]\n",
    "graph = graph.add_self_loop()\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feat, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feat)\n",
    "        self.conv2 = GraphConv(h_feat, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feat, num_classes):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feat, \"mean\")\n",
    "        self.conv2 = SAGEConv(h_feat, num_classes, \"mean\")\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv1): GraphConv(in=200, out=400, normalization=both, activation=None)\n",
       "  (conv2): GraphConv(in=400, out=6, normalization=both, activation=None)\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GCN(graph.ndata['feat'].shape[1], 400, dataset.num_classes)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphSAGE(\n",
       "  (conv1): SAGEConv(\n",
       "    (feat_drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc_self): Linear(in_features=200, out_features=400, bias=False)\n",
       "    (fc_neigh): Linear(in_features=200, out_features=400, bias=False)\n",
       "  )\n",
       "  (conv2): SAGEConv(\n",
       "    (feat_drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc_self): Linear(in_features=400, out_features=6, bias=False)\n",
       "    (fc_neigh): Linear(in_features=400, out_features=6, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GraphSAGE(graph.ndata['feat'].shape[1], 400, dataset.num_classes)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g, model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    best_test_acc = 0\n",
    "\n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    test_mask = g.ndata['test_mask']\n",
    "    label_mask = g.ndata['label_mask']\n",
    "    train_label_mask = g.ndata['train_label_mask']\n",
    "    test_label_mask = g.ndata['test_label_mask']\n",
    "    for e in range(500):\n",
    "        # Forward\n",
    "        logits = model(g, features.float())\n",
    "\n",
    "        # Compute prediction\n",
    "        pred = logits.argmax(1)\n",
    "\n",
    "        # Compute loss\n",
    "        # Note that you should only compute the losses of the nodes in the training set.\n",
    "        loss = F.cross_entropy(logits[train_label_mask], labels[train_label_mask])\n",
    "\n",
    "        # Compute accuracy on training/validation/test\n",
    "        train_acc = (pred[train_label_mask] == labels[train_label_mask]).float().mean()\n",
    "        test_acc = (pred[test_label_mask] == labels[test_label_mask]).float().mean()\n",
    "        train_f1 = f1_score(labels[train_label_mask], pred[train_label_mask], average='weighted')\n",
    "        test_f1 = f1_score(labels[test_label_mask], pred[test_label_mask], average='weighted')\n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_test_acc < test_acc:\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if e % 5 == 0:\n",
    "            print('In epoch {}, loss: {:.3f}, train_acc: {:.3f}, test acc: {:.3f} (best {:.3f}), train_f1: {:.3f}, test_f1: {:.3f}'.format(\n",
    "                e, loss, train_acc, test_acc, best_test_acc, train_f1, test_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 1.810, train_acc: 0.021, test acc: 0.017 (best 0.017), train_f1: 0.023, test_f1: 0.013\n",
      "In epoch 5, loss: 1.731, train_acc: 0.574, test acc: 0.579 (best 0.579), train_f1: 0.513, test_f1: 0.520\n",
      "In epoch 10, loss: 1.652, train_acc: 0.633, test acc: 0.645 (best 0.645), train_f1: 0.572, test_f1: 0.587\n",
      "In epoch 15, loss: 1.566, train_acc: 0.643, test acc: 0.660 (best 0.660), train_f1: 0.581, test_f1: 0.602\n",
      "In epoch 20, loss: 1.472, train_acc: 0.646, test acc: 0.663 (best 0.663), train_f1: 0.585, test_f1: 0.605\n",
      "In epoch 25, loss: 1.374, train_acc: 0.648, test acc: 0.662 (best 0.664), train_f1: 0.587, test_f1: 0.604\n",
      "In epoch 30, loss: 1.283, train_acc: 0.647, test acc: 0.666 (best 0.667), train_f1: 0.586, test_f1: 0.608\n",
      "In epoch 35, loss: 1.208, train_acc: 0.648, test acc: 0.664 (best 0.667), train_f1: 0.587, test_f1: 0.607\n",
      "In epoch 40, loss: 1.149, train_acc: 0.649, test acc: 0.664 (best 0.667), train_f1: 0.588, test_f1: 0.607\n",
      "In epoch 45, loss: 1.104, train_acc: 0.649, test acc: 0.668 (best 0.668), train_f1: 0.588, test_f1: 0.610\n",
      "In epoch 50, loss: 1.067, train_acc: 0.649, test acc: 0.671 (best 0.671), train_f1: 0.590, test_f1: 0.613\n",
      "In epoch 55, loss: 1.035, train_acc: 0.652, test acc: 0.667 (best 0.671), train_f1: 0.594, test_f1: 0.610\n",
      "In epoch 60, loss: 1.007, train_acc: 0.657, test acc: 0.666 (best 0.671), train_f1: 0.605, test_f1: 0.612\n",
      "In epoch 65, loss: 0.983, train_acc: 0.664, test acc: 0.670 (best 0.671), train_f1: 0.620, test_f1: 0.621\n",
      "In epoch 70, loss: 0.962, train_acc: 0.670, test acc: 0.675 (best 0.675), train_f1: 0.630, test_f1: 0.631\n",
      "In epoch 75, loss: 0.943, train_acc: 0.677, test acc: 0.683 (best 0.683), train_f1: 0.642, test_f1: 0.643\n",
      "In epoch 80, loss: 0.926, train_acc: 0.682, test acc: 0.686 (best 0.686), train_f1: 0.650, test_f1: 0.650\n",
      "In epoch 85, loss: 0.910, train_acc: 0.687, test acc: 0.688 (best 0.688), train_f1: 0.657, test_f1: 0.654\n",
      "In epoch 90, loss: 0.895, train_acc: 0.691, test acc: 0.691 (best 0.691), train_f1: 0.664, test_f1: 0.660\n",
      "In epoch 95, loss: 0.883, train_acc: 0.696, test acc: 0.696 (best 0.696), train_f1: 0.669, test_f1: 0.667\n",
      "In epoch 100, loss: 0.871, train_acc: 0.700, test acc: 0.699 (best 0.699), train_f1: 0.674, test_f1: 0.671\n",
      "In epoch 105, loss: 0.860, train_acc: 0.705, test acc: 0.699 (best 0.700), train_f1: 0.681, test_f1: 0.672\n",
      "In epoch 110, loss: 0.851, train_acc: 0.707, test acc: 0.700 (best 0.701), train_f1: 0.684, test_f1: 0.673\n",
      "In epoch 115, loss: 0.842, train_acc: 0.710, test acc: 0.703 (best 0.703), train_f1: 0.687, test_f1: 0.677\n",
      "In epoch 120, loss: 0.834, train_acc: 0.712, test acc: 0.703 (best 0.704), train_f1: 0.689, test_f1: 0.677\n",
      "In epoch 125, loss: 0.827, train_acc: 0.715, test acc: 0.705 (best 0.705), train_f1: 0.693, test_f1: 0.679\n",
      "In epoch 130, loss: 0.821, train_acc: 0.717, test acc: 0.706 (best 0.706), train_f1: 0.695, test_f1: 0.681\n",
      "In epoch 135, loss: 0.815, train_acc: 0.718, test acc: 0.709 (best 0.709), train_f1: 0.696, test_f1: 0.684\n",
      "In epoch 140, loss: 0.810, train_acc: 0.719, test acc: 0.706 (best 0.709), train_f1: 0.697, test_f1: 0.681\n",
      "In epoch 145, loss: 0.805, train_acc: 0.720, test acc: 0.706 (best 0.709), train_f1: 0.698, test_f1: 0.682\n",
      "In epoch 150, loss: 0.800, train_acc: 0.721, test acc: 0.705 (best 0.709), train_f1: 0.700, test_f1: 0.682\n",
      "In epoch 155, loss: 0.797, train_acc: 0.722, test acc: 0.705 (best 0.709), train_f1: 0.701, test_f1: 0.681\n",
      "In epoch 160, loss: 0.793, train_acc: 0.722, test acc: 0.704 (best 0.709), train_f1: 0.701, test_f1: 0.681\n",
      "In epoch 165, loss: 0.789, train_acc: 0.723, test acc: 0.706 (best 0.709), train_f1: 0.702, test_f1: 0.683\n",
      "In epoch 170, loss: 0.786, train_acc: 0.724, test acc: 0.706 (best 0.709), train_f1: 0.704, test_f1: 0.683\n",
      "In epoch 175, loss: 0.783, train_acc: 0.725, test acc: 0.707 (best 0.709), train_f1: 0.705, test_f1: 0.685\n",
      "In epoch 180, loss: 0.781, train_acc: 0.726, test acc: 0.707 (best 0.709), train_f1: 0.706, test_f1: 0.685\n",
      "In epoch 185, loss: 0.778, train_acc: 0.726, test acc: 0.708 (best 0.709), train_f1: 0.707, test_f1: 0.686\n",
      "In epoch 190, loss: 0.776, train_acc: 0.728, test acc: 0.708 (best 0.709), train_f1: 0.708, test_f1: 0.686\n",
      "In epoch 195, loss: 0.774, train_acc: 0.729, test acc: 0.709 (best 0.710), train_f1: 0.710, test_f1: 0.688\n",
      "In epoch 200, loss: 0.771, train_acc: 0.729, test acc: 0.709 (best 0.710), train_f1: 0.710, test_f1: 0.689\n",
      "In epoch 205, loss: 0.769, train_acc: 0.729, test acc: 0.710 (best 0.711), train_f1: 0.711, test_f1: 0.690\n",
      "In epoch 210, loss: 0.767, train_acc: 0.730, test acc: 0.710 (best 0.711), train_f1: 0.712, test_f1: 0.691\n",
      "In epoch 215, loss: 0.765, train_acc: 0.730, test acc: 0.708 (best 0.711), train_f1: 0.712, test_f1: 0.690\n",
      "In epoch 220, loss: 0.764, train_acc: 0.730, test acc: 0.707 (best 0.711), train_f1: 0.713, test_f1: 0.690\n",
      "In epoch 225, loss: 0.762, train_acc: 0.729, test acc: 0.707 (best 0.711), train_f1: 0.712, test_f1: 0.691\n",
      "In epoch 230, loss: 0.760, train_acc: 0.729, test acc: 0.707 (best 0.711), train_f1: 0.712, test_f1: 0.690\n",
      "In epoch 235, loss: 0.759, train_acc: 0.729, test acc: 0.708 (best 0.711), train_f1: 0.713, test_f1: 0.691\n",
      "In epoch 240, loss: 0.757, train_acc: 0.730, test acc: 0.707 (best 0.711), train_f1: 0.713, test_f1: 0.691\n",
      "In epoch 245, loss: 0.756, train_acc: 0.730, test acc: 0.708 (best 0.711), train_f1: 0.713, test_f1: 0.692\n",
      "In epoch 250, loss: 0.754, train_acc: 0.730, test acc: 0.708 (best 0.711), train_f1: 0.713, test_f1: 0.692\n",
      "In epoch 255, loss: 0.753, train_acc: 0.730, test acc: 0.708 (best 0.711), train_f1: 0.714, test_f1: 0.692\n",
      "In epoch 260, loss: 0.752, train_acc: 0.730, test acc: 0.708 (best 0.711), train_f1: 0.714, test_f1: 0.691\n",
      "In epoch 265, loss: 0.750, train_acc: 0.731, test acc: 0.708 (best 0.711), train_f1: 0.715, test_f1: 0.692\n",
      "In epoch 270, loss: 0.749, train_acc: 0.731, test acc: 0.708 (best 0.711), train_f1: 0.715, test_f1: 0.693\n",
      "In epoch 275, loss: 0.748, train_acc: 0.731, test acc: 0.708 (best 0.711), train_f1: 0.715, test_f1: 0.693\n",
      "In epoch 280, loss: 0.746, train_acc: 0.731, test acc: 0.710 (best 0.711), train_f1: 0.715, test_f1: 0.694\n",
      "In epoch 285, loss: 0.745, train_acc: 0.732, test acc: 0.708 (best 0.711), train_f1: 0.716, test_f1: 0.692\n",
      "In epoch 290, loss: 0.744, train_acc: 0.732, test acc: 0.708 (best 0.711), train_f1: 0.716, test_f1: 0.693\n",
      "In epoch 295, loss: 0.743, train_acc: 0.732, test acc: 0.707 (best 0.711), train_f1: 0.716, test_f1: 0.692\n",
      "In epoch 300, loss: 0.742, train_acc: 0.732, test acc: 0.707 (best 0.711), train_f1: 0.716, test_f1: 0.692\n",
      "In epoch 305, loss: 0.740, train_acc: 0.732, test acc: 0.706 (best 0.711), train_f1: 0.717, test_f1: 0.691\n",
      "In epoch 310, loss: 0.739, train_acc: 0.733, test acc: 0.705 (best 0.711), train_f1: 0.717, test_f1: 0.690\n",
      "In epoch 315, loss: 0.738, train_acc: 0.733, test acc: 0.705 (best 0.711), train_f1: 0.718, test_f1: 0.689\n",
      "In epoch 320, loss: 0.737, train_acc: 0.733, test acc: 0.704 (best 0.711), train_f1: 0.718, test_f1: 0.688\n",
      "In epoch 325, loss: 0.736, train_acc: 0.733, test acc: 0.704 (best 0.711), train_f1: 0.719, test_f1: 0.689\n",
      "In epoch 330, loss: 0.735, train_acc: 0.734, test acc: 0.704 (best 0.711), train_f1: 0.719, test_f1: 0.689\n",
      "In epoch 335, loss: 0.734, train_acc: 0.734, test acc: 0.705 (best 0.711), train_f1: 0.720, test_f1: 0.689\n",
      "In epoch 340, loss: 0.733, train_acc: 0.735, test acc: 0.704 (best 0.711), train_f1: 0.720, test_f1: 0.689\n",
      "In epoch 345, loss: 0.731, train_acc: 0.735, test acc: 0.705 (best 0.711), train_f1: 0.720, test_f1: 0.690\n",
      "In epoch 350, loss: 0.730, train_acc: 0.735, test acc: 0.705 (best 0.711), train_f1: 0.721, test_f1: 0.690\n",
      "In epoch 355, loss: 0.729, train_acc: 0.736, test acc: 0.705 (best 0.711), train_f1: 0.722, test_f1: 0.690\n",
      "In epoch 360, loss: 0.728, train_acc: 0.736, test acc: 0.704 (best 0.711), train_f1: 0.722, test_f1: 0.689\n",
      "In epoch 365, loss: 0.727, train_acc: 0.737, test acc: 0.704 (best 0.711), train_f1: 0.722, test_f1: 0.689\n",
      "In epoch 370, loss: 0.726, train_acc: 0.737, test acc: 0.703 (best 0.711), train_f1: 0.723, test_f1: 0.688\n",
      "In epoch 375, loss: 0.725, train_acc: 0.737, test acc: 0.704 (best 0.711), train_f1: 0.723, test_f1: 0.690\n",
      "In epoch 380, loss: 0.724, train_acc: 0.737, test acc: 0.705 (best 0.711), train_f1: 0.723, test_f1: 0.690\n",
      "In epoch 385, loss: 0.723, train_acc: 0.737, test acc: 0.705 (best 0.711), train_f1: 0.723, test_f1: 0.690\n",
      "In epoch 390, loss: 0.722, train_acc: 0.738, test acc: 0.705 (best 0.711), train_f1: 0.724, test_f1: 0.691\n",
      "In epoch 395, loss: 0.721, train_acc: 0.738, test acc: 0.705 (best 0.711), train_f1: 0.724, test_f1: 0.691\n",
      "In epoch 400, loss: 0.720, train_acc: 0.738, test acc: 0.706 (best 0.711), train_f1: 0.724, test_f1: 0.691\n",
      "In epoch 405, loss: 0.718, train_acc: 0.739, test acc: 0.705 (best 0.711), train_f1: 0.725, test_f1: 0.691\n",
      "In epoch 410, loss: 0.717, train_acc: 0.739, test acc: 0.706 (best 0.711), train_f1: 0.725, test_f1: 0.692\n",
      "In epoch 415, loss: 0.716, train_acc: 0.739, test acc: 0.705 (best 0.711), train_f1: 0.726, test_f1: 0.691\n",
      "In epoch 420, loss: 0.715, train_acc: 0.740, test acc: 0.705 (best 0.711), train_f1: 0.726, test_f1: 0.691\n",
      "In epoch 425, loss: 0.714, train_acc: 0.740, test acc: 0.705 (best 0.711), train_f1: 0.726, test_f1: 0.691\n",
      "In epoch 430, loss: 0.713, train_acc: 0.740, test acc: 0.706 (best 0.711), train_f1: 0.727, test_f1: 0.692\n",
      "In epoch 435, loss: 0.712, train_acc: 0.740, test acc: 0.706 (best 0.711), train_f1: 0.727, test_f1: 0.692\n",
      "In epoch 440, loss: 0.711, train_acc: 0.740, test acc: 0.705 (best 0.711), train_f1: 0.727, test_f1: 0.691\n",
      "In epoch 445, loss: 0.710, train_acc: 0.741, test acc: 0.705 (best 0.711), train_f1: 0.727, test_f1: 0.691\n",
      "In epoch 450, loss: 0.709, train_acc: 0.741, test acc: 0.705 (best 0.711), train_f1: 0.728, test_f1: 0.692\n",
      "In epoch 455, loss: 0.707, train_acc: 0.742, test acc: 0.705 (best 0.711), train_f1: 0.728, test_f1: 0.691\n",
      "In epoch 460, loss: 0.706, train_acc: 0.742, test acc: 0.704 (best 0.711), train_f1: 0.729, test_f1: 0.691\n",
      "In epoch 465, loss: 0.705, train_acc: 0.742, test acc: 0.704 (best 0.711), train_f1: 0.729, test_f1: 0.691\n",
      "In epoch 470, loss: 0.704, train_acc: 0.742, test acc: 0.705 (best 0.711), train_f1: 0.729, test_f1: 0.692\n",
      "In epoch 475, loss: 0.703, train_acc: 0.743, test acc: 0.705 (best 0.711), train_f1: 0.730, test_f1: 0.692\n",
      "In epoch 480, loss: 0.702, train_acc: 0.744, test acc: 0.705 (best 0.711), train_f1: 0.731, test_f1: 0.691\n",
      "In epoch 485, loss: 0.701, train_acc: 0.744, test acc: 0.704 (best 0.711), train_f1: 0.731, test_f1: 0.691\n",
      "In epoch 490, loss: 0.699, train_acc: 0.745, test acc: 0.704 (best 0.711), train_f1: 0.732, test_f1: 0.691\n",
      "In epoch 495, loss: 0.698, train_acc: 0.745, test acc: 0.704 (best 0.711), train_f1: 0.733, test_f1: 0.691\n"
     ]
    }
   ],
   "source": [
    "train(graph, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.7708), tensor(5.7708))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(graph.in_degrees().float()), torch.mean(graph.out_degrees().float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 4.,  2.,  3.,  ..., 14.,  4.,  2.]),\n",
       " tensor([18.,  3.,  7.,  ...,  8.,  3.,  4.]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.in_degrees().float(), graph.out_degrees().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fb3b9acd036f0bb40a19178a4a4e0c104f45e4cf2c749b0af2e81dcdbc701d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
